---
layout: page
title: Related Work, Background, Problem Description, Preliminaries 작성 방법
permalink: /lecture/material/intro-technical-writing/related-work
image: CUK_4Seasons.jpg
description: 논문에서 본론으로 들어가기 전, Related Work(선행연구), Background(이론적 배경), Problem Description(문제 정의), Preliminaries 등을 어떻게 구성하느냐가 전체 논문의 명확성과 읽기 편의성을 크게 좌우합니다.
toc: true
toc_sticky: true
toc_label: "Table of Contents"
---

논문에서 **본론**으로 들어가기 전, **Related Work(선행연구)**, **Background(이론적 배경)**, **Problem Description(문제 정의)**, **Preliminaries** 등을 어떻게 구성하느냐가 전체 논문의 **명확성**과 **읽기 편의성**을 크게 좌우합니다.

- **Related Work**: 이 연구가 어떤 맥락에서 진행되었는지, 기존 연구 동향과 한계를 정리  
- **Background**: 문제를 이해하기 위한 **수학적/이론적 기초**, **모델 개념**, **용어** 등을 설명  
- **Problem Description**: 연구 대상 문제(챌린지)를 구체적으로 정의하거나, 실무 환경/데이터 특성을 제시  
- **Preliminaries**: 필요하다면 기존 알고리즘(예: CNN, Transformer)이나 수학 기호 등을 간단히 리마인드

일반적으로 **AI 논문**에서 이 파트는 **2장**(혹은 2장+3장) 정도에 배치되며, **서론(Introduction)**에 이어 **본격적인 연구 방법**을 설명하기 전에 독자가 **충분히 필요한 배경**을 습득할 수 있게 도와줍니다.

---

## 1. Related Work (선행연구) 작성 방법

### 1.1 AI 분야에서 선행연구 정리의 중요성

1. **연구 동향 파악**: 빠르게 변하는 AI 트렌드 속에서, 어떤 연구들이 먼저 시도되었고 어떤 결과가 나왔는지 요약해 주면, 독자(심사위원) 입장에서 “이 논문이 학문적으로 어디에 위치하는가?”를 쉽게 파악할 수 있습니다.

2. **연구 격차 확인**: “어떤 부분은 이미 잘 해결되었고, 어디가 부족한가?”를 명확히 짚어야, **본 논문**이 그 빈틈을 어떻게 메우는지 강조할 수 있습니다.

3. **중복 연구 방지**: 비슷한 접근이 이미 시도되었다면, 그와 어떻게 다른지 비교 설명해야 합니다.  
   - 탑티어 학회(NeurIPS, ICML, CVPR 등) 심사 시, **이미 나온 방법과 큰 차이가 없다**고 판단되면 곤란하겠죠.

### 1.2 정리 방식

- **주제별/접근법별**로 분류  
  - 예: “(1) CNN 기반 모델, (2) Transformer 기반 모델, (3) Hybrid 접근”으로 나누어 정리  
  - 혹은 “(1) 데이터 중심 연구, (2) 알고리즘/모델 개선, (3) 응용/시스템 연구” 방식도 가능

- **연대순**(과거 → 최근 순) 정리는 방대해질 때 유리하지만, AI 논문에서는 **주제별** 분류가 이해하기 훨씬 쉽습니다.

### 1.3 예시 문단 흐름

**예시**:  
1. **CNN 기반 접근**: LeCun 등(1998) 이후 CNN은 이미지 처리에서 압도적 성능을 보여왔다. 이후 ResNet(He et al., 2016), DenseNet 등 고도화된 아키텍처가 제시되었으며, 주로 시각적 객체 인식 등에 적용되었다. 하지만 고해상도 영상 처리 시 모델 파라미터와 연산량 문제가 크게 증가한다는 단점이 지적되었다.  
2. **Transformer 기반 접근**: Vaswani 등(2017)의 Transformer는 원래 NLP에서 제안되었지만, Vision Transformer(Dosovitskiy 등, 2021) 등의 연구를 통해 이미지 영역에도 성공적으로 적용되었다. 그러나 대규모 데이터셋에서 학습해야 성능이 충분히 발휘되며, 연산 비용이 상당히 크다는 지적이 있다.  
3. **하이브리드·경량화 시도**: 최근에는 CNN과 Transformer 요소를 혼합하여 장점을 결합하거나, 연산량을 줄이는 Light-Transformer 기법(Lee 등, 2022) 등이 제시되고 있다. 아직 고해상도 이미지 생성/분석 분야에서는 제한적 연구만 존재한다.

이처럼, **각 그룹(접근법)** 별로 “어떤 연구가 있었고, 무엇이 부족한지”를 간략히 서술해 주면 됩니다.

---

## 2. Background (이론적 배경) 작성 방법

### 2.1 언제 Background가 필요한가?

- **수학적 공식/개념**이 중요한 논문(예: Reinforcement Learning의 Markov Decision Process, Variational Inference, Graph Theory 등)에서는 별도의 Background 섹션이 유용합니다.  
- 독자가 이 논문을 이해하기 위해 **필수적인 공식, 정의, 알고리즘**을 미리 익히도록 돕는 역할을 합니다.

### 2.2 포함할 내용 예시

- **기호 정의**: 수학 기호, 변수, 벡터/행렬 표기, 확률 분포 등  
- **주요 정리(정역)**: 본 연구에서 사용하는 특정 이론(예: Jensen’s Inequality, KL Divergence, Bellman Equation 등)  
- **베이스 모델 소개**: “우리는 기존 BERT 모델에서 Start/End token 처리를 변경했다”처럼, **기존 모델**에 대한 간단한 설명

### 2.3 작성 요령

1. **간결**: 필요한 부분만 요약하고, 교과서 수준의 상세 설명은 지양(“더 자세한 내용은 OOO 교재 2장 참조” 식으로 처리 가능).  
2. **인용**: “이 알고리즘은 (Kingma & Ba, 2015)의 Adam Optimizer에 기반한다”처럼, **원 논문**에 대한 인용을 빼먹지 말기.  
3. **명확한 연결**: Background 후반에 **“따라서 본 논문에서는 ~개념을 확장하여 사용한다”** 식의 문장으로 **본 논문**과의 연계성을 보여주면 좋습니다.

---

## 3. Problem Description (문제 정의) 작성 방법

### 3.1 AI 논문에서 문제 정의의 역할

- **연구 대상 문제**를 정확히 규정함으로써, **추상적 목표**가 아닌 **구체적 해결 과제**로 독자의 이해를 돕는 역할을 합니다.  
- 예: “본 논문은 다음과 같은 문제를 해결하고자 한다: 고해상도 이미지를 입력받아, 지정된 목표 객체가 포함된 영역을 세밀하게 분할(Segmentation)하는 모델을 학습한다.”

### 3.2 작성 시 포함할 내용

1. **입력(Input)**: 어떤 형태의 데이터를 받는가? (이미지, 텍스트, 시계열, 그래프 등)  
2. **출력(Output)**: 모델이 최종적으로 예측/생성해야 하는 대상(클래스 라벨, 확률 분포, 문장 등)  
3. **목적함수(Objective)**: 손실 함수, 최적화 목표(예: MSE, Cross-Entropy, IoU, BLEU Score 등)  
4. **제약조건**: 데이터 크기, 실시간 처리 요구, 메모리 제한, 프라이버시 등  
5. **평가 지표(Evaluation Metric)**: Accuracy, F1-score, BLEU, Mean IoU, PSNR, RMSE 등

### 3.3 예시 문단

**Problem Description (예시)**  
“이 연구에서 다루는 문제는 **의료영상에서 종양 영역을 자동 세분화**하는 것이다. 입력은 (1) 환자 CT 스캔 이미지, (2) 환자의 기초 메타데이터(나이, 성별 등)이며, 모델은 이 이미지를 통해 종양의 영역을 픽셀 단위로 구분(segmentation)해야 한다.  
평가 시 Dice Similarity Coefficient(DSC)를 핵심 지표로 활용하며, 0.9 이상이 나오면 임상적으로 의미 있는 정확도로 간주한다. 연산 시간은 환자 1명 처리 기준 5초 이내여야 하며, GPU 메모리 사용량은 8GB 이내로 제한한다.”

---

## 4. Preliminaries 작성 방법

### 4.1 Preliminaries vs. Background 차이

- **Background**는 보통 **이론적/수학적** 개념이나 **기존 알고리즘** 위주.  
- **Preliminaries**는 **보다 구체적인 작업 환경**이나 **데이터셋**, **학회/도메인 표준** 등을 정의하는 경우가 많습니다.  
  - 예: “본 논문의 모든 실험은 PyTorch 1.10 버전과 CUDA 11.3 환경에서 진행되었다” / “하이퍼파라미터 튜닝은 Optuna로 자동화했다” 등  
  - 혹은 “그래프 신경망의 기본 연산을 간단히 요약하면 …” 처럼, AI 분야에서 일반적으로 알고 있다고 가정하는 기초 개념을 상기시켜주는 것도 Preliminaries에 포함될 수 있습니다.

### 4.2 AI 논문에서 주로 다루는 Preliminaries

- **데이터셋 구성**: 얼마나 많은 샘플이 있고, 어떤 라벨 정보가 있는지  
- **실험 프로토콜**: 학습/검증/테스트 분할 방법, 랜덤 시드 설정, 크로스 밸리데이션 여부 등  
- **하드웨어/라이브러리 정보**: GPU/TPU 사양, Python 패키지 버전, 특정 오픈소스 라이브러리  
- **수식 기호 정의**(Background와 중복 방지하며 필요 최소만)

---

## 5. 작성 시 주의 사항

1. **중복 배제**: 서론, Related Work, Background에서 이미 설명한 내용을 여기서 또 자세히 반복하지 않도록 주의.  
2. **논문 구조**: 꼭 ‘Related Work → Background → Problem Description → Preliminaries’ 순서대로 넣을 필요는 없습니다.  
   - 필요에 따라 ‘Related Work & Background’를 합치는 경우도 많습니다.  
   - AI 컨퍼런스(NeurIPS, ICML) 논문 구조는 대체로 페이지 제한이 있으므로, 섹션 간략화가 흔합니다.
3. **필요성**: **AI 논문 심사**에서는 “불필요하게 장황한 설명”을 싫어합니다. 독자 이해에 중요한 배경/이론만 간추려서 제시하세요.  
4. **적절한 인용**: 이전 연구, 기존 알고리즘, 데이터셋 출처 등은 반드시 정확히 레퍼런스를 달아주세요(“본인이 만든 것”처럼 착각을 주면 안 됩니다).

---

## 6. 결론

**Related Work / Background / Problem Description / Preliminaries** 파트는 **AI 논문의 핵심 컨텐츠**가 제대로 자리 잡기 전에, **읽는 이가 반드시 알고 넘어가야 할 사항**을 깔끔하게 정리해 주는 역할을 합니다. 잘 정리된 이 섹션들은 **메인 아이디어(제안 기법, 실험 결과 등)**를 더욱 빛나게 만들어 줍니다.

1. **Related Work**: 기존 연구 흐름과 공백(Gap)을 짚어, 본 논문의 기여를 도드라지게  
2. **Background**: 필요한 수학/이론/알고리즘을 간략하고 명확하게 정리  
3. **Problem Description**: 연구 대상 문제를 **입력-출력-제약-지표** 등으로 구체화  
4. **Preliminaries**: 실험 환경, 데이터셋, 라이브러리, 용어 정의 등 사전 지식

---

## 참고자료

- *Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.*  
- *NeurIPS, ICML, ICLR 등 AI 학회 저자 가이드*  
- *Overleaf: Sections, Subsections & Citations Tutorial*

---

